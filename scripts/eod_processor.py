#!/usr/bin/env python3
"""
EOD CSVä¸“ä¸šå¤„ç†å™¨ - Pythonç‰ˆ
åŠŸèƒ½ï¼šåˆ—é‡æ–°æ’åº + è¡Œä¸šä»£ç è½¬æ¢
å¯¹åº”HTMLç‰ˆæœ¬çš„æ‰€æœ‰åŠŸèƒ½
"""

import pandas as pd
import numpy as np
import sys
import os
import json
from datetime import datetime
import argparse
import re

# ============================================================================
# é…ç½®æ•°æ®
# ============================================================================

# æ ‡å‡†åˆ—é¡ºåº
STANDARD_COLUMNS = [
    "Code", "Stock", "Sector", "Open", "Last", "Prv Close", "Chg", "High", "Low", 
    "Y-High", "Y-Low", "Vol", "DY*", "B%", "Vol MA (20)", "RSI (14)", "MACD (26,12)", 
    "EPS*", "P/E", "Status"
]

# ä¼˜åŒ–çš„åˆ—åæ˜ å°„ï¼ˆæ”¯æŒå¤šç§åˆ—åå˜ä½“ï¼‰
COLUMN_MAPPING = {
    "Code": ["Code", "è‚¡ç¥¨ä»£ç ", "ä»£ç ", "Symbol", "Ticker", "ä»£å·", "è¯åˆ¸ä»£ç ", "è‚¡å·"],
    "Stock": ["Stock", "è‚¡ç¥¨", "åç§°", "Name", "å…¬å¸åç§°", "è‚¡ç¥¨åç§°", "å…¬å¸", "è‚¡ç¥¨å"],
    "Sector": ["Sector", "è¡Œä¸š", "æ¿å—", "Industry", "è¡Œä¸šåˆ†ç±»", "æ‰€å±è¡Œä¸š", "äº§ä¸š", "æ¿å—åˆ†ç±»"],
    "Open": ["Open", "å¼€ç›˜ä»·", "å¼€ç›˜", "Opening Price", "å¼€å¸‚ä»·", "ä»Šå¼€", "å¼€ç›˜ä»·æ ¼"],
    "Last": ["Last", "æœ€æ–°ä»·", "ç°ä»·", "å½“å‰ä»·", "æ”¶ç›˜ä»·", "æœ€åä»·", "æˆäº¤ä»·", "å½“å‰ä»·æ ¼"],
    "Prv Close": ["Prv Close", "å‰æ”¶ç›˜", "æ˜¨æ—¥æ”¶ç›˜", "Previous Close", "å‰æ”¶", "æ˜¨æ”¶", "å‰ä¸€æ—¥æ”¶ç›˜", "æ˜¨æ—¥æ”¶å¸‚", "Prev Close"],
    "Chg": ["Chg", "æ¶¨è·Œ", "å˜åŒ–", "Change", "æ¶¨è·Œå¹…", "å˜åŠ¨", "æ¶¨å¹…", "å˜åŒ–ç‡", "æ¶¨è·Œ%", "Chg%", "Change%"],
    "High": ["High", "æœ€é«˜ä»·", "æœ€é«˜", "æœ€é«˜ä»·", "æ—¥å†…æœ€é«˜", "å½“æ—¥æœ€é«˜"],
    "Low": ["Low", "æœ€ä½ä»·", "æœ€ä½", "æœ€ä½ä»·", "æ—¥å†…æœ€ä½", "å½“æ—¥æœ€ä½"],
    "Y-High": ["Y-High", "å¹´æœ€é«˜", "52å‘¨æœ€é«˜", "Year High", "52å‘¨é«˜", "å¹´åº¦æœ€é«˜", "å¹´å†…æœ€é«˜", "Year-High"],
    "Y-Low": ["Y-Low", "å¹´æœ€ä½", "52å‘¨æœ€ä½", "Year Low", "52å‘¨ä½", "å¹´åº¦æœ€ä½", "å¹´å†…æœ€ä½", "Year-Low"],
    "Vol": ["Vol", "æˆäº¤é‡", "äº¤æ˜“é‡", "Volume", "æˆäº¤é¢", "é‡", "æˆäº¤è‚¡æ•°", "äº¤æ˜“è‚¡æ•°"],
    "DY*": ["DY*", "è‚¡æ¯ç‡", "è‚¡æ¯æ”¶ç›Šç‡", "Dividend Yield", "è‚¡æ¯", "åˆ†çº¢ç‡", "è‚¡æ¯%", "Dividend"],
    "B%": ["B%", "è´å¡”ç³»æ•°", "Beta", "æ³¢åŠ¨ç‡", "é£é™©ç³»æ•°", "Î²", "Betaç³»æ•°"],
    "Vol MA (20)": ["Vol MA (20)", "æˆäº¤é‡å‡çº¿20", "20æ—¥æˆäº¤é‡å‡çº¿", "Vol MA 20", "Volume MA 20", "20æ—¥å‡é‡", "æˆäº¤é‡20æ—¥å‡çº¿", "Vol MA(20)"],
    "RSI (14)": ["RSI (14)", "RSI", "ç›¸å¯¹å¼ºå¼±æŒ‡æ•°", "RSI 14", "ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡", "RSIæŒ‡æ ‡"],
    "MACD (26,12)": ["MACD (26,12)", "MACD", "æŒ‡æ•°å¹³æ»‘å¼‚åŒç§»åŠ¨å¹³å‡çº¿", "MACDæŒ‡æ ‡", 
                    "MACD(26,12)", "MACD (26, 12)", "MACD(26, 12)", "MACD 26 12", 
                    "MACD(26,12,9)", "MACD (26,12,9)", "MACD 26-12"],
    "EPS*": ["EPS*", "æ¯è‚¡æ”¶ç›Š", "EPS", "æ¯è‚¡ç›ˆåˆ©", "æ¯è‚¡ç›ˆä½™", "Earnings Per Share", "æ¯è‚¡æ”¶ç›ŠEPS"],
    "P/E": ["P/E", "å¸‚ç›ˆç‡", "PE", "è‚¡ä»·æ”¶ç›Šæ¯”", "æœ¬ç›Šæ¯”", "å¸‚ç›ˆç‡(PE)", "PE Ratio", "P/E Ratio"],
    "Status": ["Status", "çŠ¶æ€", "äº¤æ˜“çŠ¶æ€", "ä¸Šå¸‚çŠ¶æ€", "è‚¡ç¥¨çŠ¶æ€", "ä¸Šå¸‚æƒ…å†µ", "äº¤æ˜“æƒ…å†µ"]
}

# è¡Œä¸šæ˜ å°„æ•°æ®
SECTOR_MAP = {
    "101": "Industrial & Consumer Products",
    "102": "Industrial & Consumer Products", 
    "103": "Industrial & Consumer Products",
    "105": "Industrial & Consumer Products",
    "110": "Industrial & Consumer Products",
    "120": "Industrial & Consumer Products",
    "125": "Industrial & Consumer Products",
    "150": "Industrial & Consumer Products",
    "155": "Industrial & Consumer Products",
    "161": "Industrial & Consumer Products",
    "162": "Industrial & Consumer Products",
    "163": "Industrial & Consumer Products",
    "164": "Industrial & Consumer Products",
    "165": "Industrial & Consumer Products",
    "166": "Industrial & Consumer Products",
    "301": "Technology",
    "302": "Technology",
    "303": "Technology",
    "305": "Technology",
    "310": "Technology",
    "320": "Technology",
    "325": "Technology",
    "358": "Technology",
    "361": "Technology",
    "362": "Technology",
    "363": "Technology",
    "364": "Technology",
    "365": "Technology",
    "401": "Property",
    "402": "Property",
    "403": "Property",
    "405": "Property",
    "410": "Property",
    "420": "Property",
    "425": "Property",
    "461": "Property",
    "462": "Property",
    "463": "Property",
    "464": "Property",
    "465": "Property",
    "501": "Telecommunications & Media",
    "502": "Telecommunications & Media",
    "520": "Telecommunications & Media",
    "560": "Telecommunications & Media",
    "653": "Transportation & Logistics",
    "654": "Transportation & Logistics",
    "656": "Transportation & Logistics",
    "657": "Transportation & Logistics",
    "701": "Utilities",
    "702": "Utilities",
    "703": "Utilities",
    "705": "Utilities",
    "710": "Utilities",
    "725": "Utilities",
    "762": "Utilities",
    "0162": "Medical Devices & Supplies",
    "0405": "Software & IT Services",
    "1701": "Industrial Holding Firms",
    "1702": "Industrial & Consumer Products",
    "1703": "Industrial Support Services",
    "1704": "Building Materials",
    "1705": "Construction & Infrastructure",
    "1706": "Transportation & Logistics",
    "1801": "Consumer Product Holding Firms",
    "1802": "Food, Beverage & Tobacco",
    "1803": "Retail & Distribution",
    "1804": "Hotel, Resort & Recreational Services",
    "1805": "Media & Entertainment",
    "1806": "Other Consumer Services",
    "1807": "Health Care Equipment & Services",
    "1808": "Pharmaceuticals & Biotechnology",
    "1809": "Technology",
    "1810": "Telecommunications & Media",
    "0200": "Plantation",
    "0501": "Property Holding Firms",
    "0502": "Property Development",
    "0503": "Real Estate Investment Trusts (REITs)",
    "0504": "Other Property-related Services",
    "1201": "Financial Holding Firms",
    "1202": "Commercial Banks",
    "1203": "Insurance",
    "1204": "Investment Banks",
    "1205": "Other Finance",
    "0301": "Energy Holding Firms",
    "0302": "Energy-related Equipment & Services",
    "0303": "Oil & Gas",
    "0401": "Utilities Holding Firms",
    "0402": "Gas, Water & Multi-utilities",
    "0403": "Electricity",
    "0080": "Special Purpose Acquisition",
    
    # é»˜è®¤æ˜ å°„ï¼ˆæ•°å­—ä»£ç è½¬è¡Œä¸šï¼‰
    "1": "Industrial & Consumer Products",
    "2": "Technology", 
    "3": "Property",
    "4": "Telecommunications & Media",
    "5": "Transportation & Logistics",
    "6": "Utilities",
    "7": "Medical",
    "8": "Financial",
    "9": "Energy",
    "10": "Consumer"
}

# ============================================================================
# æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ============================================================================

def check_column_match(actual_col, standard_col):
    """
    æ£€æŸ¥å®é™…åˆ—åæ˜¯å¦åŒ¹é…æ ‡å‡†åˆ—å
    è¿”å›åŒ¹é…åˆ†æ•°ï¼ˆ0-10ï¼‰
    """
    if not actual_col or not standard_col:
        return 0
    
    # æ¸…ç†åˆ—å
    clean_actual = str(actual_col).strip().replace('\ufeff', '').lower()
    clean_standard = str(standard_col).strip().lower()
    
    # 1. å®Œå…¨åŒ¹é…ï¼ˆ10åˆ†ï¼‰
    if clean_actual == clean_standard:
        return 10
    
    # 2. æ£€æŸ¥åˆ—åæ˜ å°„ï¼ˆ8åˆ†ï¼‰
    if standard_col in COLUMN_MAPPING:
        for variant in COLUMN_MAPPING[standard_col]:
            if clean_actual == variant.lower():
                return 8
            if clean_actual in variant.lower() or variant.lower() in clean_actual:
                return 7
    
    # 3. å¤„ç†ä¸­æ–‡åˆ—åï¼ˆ7åˆ†ï¼‰
    chinese_mapping = {
        "Code": ["ä»£ç ", "ä»£å·", "è‚¡å·"],
        "Stock": ["è‚¡ç¥¨", "åç§°"],
        "Sector": ["è¡Œä¸š"],
        "Open": ["å¼€ç›˜ä»·", "å¼€ç›˜"],
        "Last": ["æœ€æ–°ä»·", "æ”¶ç›˜ä»·"],
        "Prv Close": ["å‰æ”¶ç›˜", "æ˜¨æ”¶"],
        "Chg": ["æ¶¨è·Œ", "æ¶¨è·Œå¹…", "å˜åŒ–"],
        "High": ["æœ€é«˜ä»·", "æœ€é«˜"],
        "Low": ["æœ€ä½ä»·", "æœ€ä½"],
        "Y-High": ["å¹´æœ€é«˜", "52å‘¨æœ€é«˜"],
        "Y-Low": ["å¹´æœ€ä½", "52å‘¨æœ€ä½"],
        "Vol": ["æˆäº¤é‡", "äº¤æ˜“é‡"],
        "DY*": ["è‚¡æ¯ç‡", "è‚¡æ¯æ”¶ç›Šç‡"],
        "B%": ["è´å¡”ç³»æ•°", "Beta"],
        "Vol MA (20)": ["æˆäº¤é‡å‡çº¿20", "20æ—¥æˆäº¤é‡å‡çº¿"],
        "RSI (14)": ["RSI", "ç›¸å¯¹å¼ºå¼±æŒ‡æ•°"],
        "MACD (26,12)": ["MACD", "æŒ‡æ•°å¹³æ»‘å¼‚åŒç§»åŠ¨å¹³å‡çº¿"],
        "EPS*": ["æ¯è‚¡æ”¶ç›Š", "EPS"],
        "P/E": ["å¸‚ç›ˆç‡", "PE"],
        "Status": ["çŠ¶æ€", "äº¤æ˜“çŠ¶æ€"]
    }
    
    if standard_col in chinese_mapping:
        for chinese in chinese_mapping[standard_col]:
            if chinese in actual_col or actual_col in chinese:
                return 7
    
    # 4. å…³é”®è¯åŒ¹é…ï¼ˆ6åˆ†ï¼‰
    standard_words = re.findall(r'[a-zA-Z0-9]+', clean_standard)
    actual_words = re.findall(r'[a-zA-Z0-9]+', clean_actual)
    
    for word in standard_words:
        if len(word) > 2 and word in clean_actual:
            return 6
    
    # 5. éƒ¨åˆ†åŒ¹é…ï¼ˆ4åˆ†ï¼‰
    for word in standard_words:
        if len(word) > 3:
            for actual_word in actual_words:
                if len(actual_word) > 3:
                    if word in actual_word or actual_word in word:
                        return 4
    
    # 6. å®Œå…¨æ— åŒ¹é…ï¼ˆ0åˆ†ï¼‰
    return 0

def auto_align_columns(df_columns):
    """
    è‡ªåŠ¨å¯¹é½åˆ—åˆ°æ ‡å‡†é¡ºåº
    è¿”å›ï¼š(target_order, mapping_info, match_score)
    """
    target_order = []
    used_columns = []
    mapping_info = []
    total_score = 0
    
    print("ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ—åŒ¹é…...")
    
    # ä¸ºæ¯ä¸ªæ ‡å‡†åˆ—å¯»æ‰¾æœ€ä½³åŒ¹é…
    for std_col in STANDARD_COLUMNS:
        best_match = None
        best_score = 0
        
        for actual_col in df_columns:
            if actual_col in used_columns:
                continue
            
            score = check_column_match(actual_col, std_col)
            
            if score > best_score:
                best_score = score
                best_match = actual_col
        
        # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œæ·»åŠ åˆ°ç›®æ ‡é¡ºåº
        if best_match and best_score >= 4:
            target_order.append(best_match)
            used_columns.append(best_match)
            mapping_info.append({
                "standard": std_col,
                "actual": best_match,
                "score": best_score,
                "status": "âœ“ åŒ¹é…" if best_score >= 6 else "âš  éƒ¨åˆ†åŒ¹é…"
            })
            total_score += best_score
            print(f"  {std_col:15} -> {best_match:20} ({best_score}/10)")
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œæ·»åŠ æ ‡å‡†åˆ—åï¼ˆç•™ç©ºï¼‰
            target_order.append(std_col)
            mapping_info.append({
                "standard": std_col,
                "actual": None,
                "score": 0,
                "status": "âœ— æœªåŒ¹é…"
            })
            print(f"  {std_col:15} -> {'[æœªåŒ¹é…]':20} (0/10)")
    
    # æ·»åŠ å‰©ä½™çš„åˆ—
    for actual_col in df_columns:
        if actual_col not in used_columns:
            target_order.append(actual_col)
            mapping_info.append({
                "standard": "(é¢å¤–)",
                "actual": actual_col,
                "score": 0,
                "status": "é¢å¤–åˆ—"
            })
    
    # è®¡ç®—åŒ¹é…ç‡
    matched_count = len([m for m in mapping_info if m['score'] >= 4])
    match_rate = (matched_count / len(STANDARD_COLUMNS)) * 100
    
    return target_order, mapping_info, match_rate

def apply_sector_mapping(df):
    """
    åº”ç”¨è¡Œä¸šä»£ç æ˜ å°„
    """
    print("ğŸ­ åº”ç”¨è¡Œä¸šä»£ç æ˜ å°„...")
    
    # æŸ¥æ‰¾Sectoråˆ—
    sector_col = None
    for col in df.columns:
        if check_column_match(col, "Sector") >= 4:
            sector_col = col
            break
    
    if not sector_col:
        print("âš   æœªæ‰¾åˆ°Sectoråˆ—ï¼Œè·³è¿‡è¡Œä¸šæ˜ å°„")
        return df
    
    # åº”ç”¨æ˜ å°„
    def map_sector(code):
        if pd.isna(code):
            return "Unknown"
        
        code_str = str(code).strip()
        
        # ç›´æ¥åŒ¹é…
        if code_str in SECTOR_MAP:
            return SECTOR_MAP[code_str]
        
        # é»˜è®¤æ˜ å°„ï¼ˆæŒ‰ç¬¬ä¸€ä¸ªæ•°å­—ï¼‰
        if code_str and code_str[0].isdigit():
            first_digit = code_str[0]
            if first_digit in SECTOR_MAP:
                return SECTOR_MAP[first_digit]
        
        # èŒƒå›´åŒ¹é…ï¼ˆ101-166ç­‰ï¼‰
        if code_str.isdigit():
            code_int = int(code_str)
            for key, value in SECTOR_MAP.items():
                if '-' in key:
                    start, end = map(int, key.split('-'))
                    if start <= code_int <= end:
                        return value
        
        return f"Unknown ({code_str})"
    
    df[sector_col] = df[sector_col].apply(map_sector)
    
    # ç»Ÿè®¡è¡Œä¸šåˆ†å¸ƒ
    sector_counts = df[sector_col].value_counts()
    print("ğŸ“Š è¡Œä¸šåˆ†å¸ƒç»Ÿè®¡:")
    for sector, count in sector_counts.head(10).items():
        percentage = (count / len(df)) * 100
        print(f"  {sector:40} {count:5} è¡Œ ({percentage:.1f}%)")
    
    return df

def reorder_dataframe(df, target_order):
    """
    æŒ‰ç…§ç›®æ ‡é¡ºåºé‡æ–°æ’åˆ—DataFrame
    """
    print("ğŸ”„ é‡æ–°æ’åˆ—æ•°æ®åˆ—...")
    
    # åˆ›å»ºåˆ—æ˜ å°„
    column_mapping = {}
    
    for std_col in STANDARD_COLUMNS:
        found_col = None
        for target_col in target_order:
            if target_col in df.columns and check_column_match(target_col, std_col) >= 4:
                found_col = target_col
                break
        
        column_mapping[std_col] = found_col
    
    # åˆ›å»ºæ–°çš„DataFrame
    new_data = {}
    
    for std_col, actual_col in column_mapping.items():
        if actual_col and actual_col in df.columns:
            new_data[std_col] = df[actual_col]
            print(f"  {std_col:15} â† {actual_col:20}")
        else:
            new_data[std_col] = pd.Series([""] * len(df))
            print(f"  {std_col:15} â† {'[ç©º]':20}")
    
    # æ·»åŠ é¢å¤–çš„åˆ—
    for col in df.columns:
        if col not in [v for v in column_mapping.values() if v]:
            new_data[col] = df[col]
            print(f"  {col:15} â† {col:20} (é¢å¤–)")
    
    result_df = pd.DataFrame(new_data)
    
    # ç¡®ä¿æ ‡å‡†åˆ—é¡ºåº
    final_columns = STANDARD_COLUMNS.copy()
    for col in df.columns:
        if col not in final_columns and col not in [v for v in column_mapping.values() if v]:
            final_columns.append(col)
    
    result_df = result_df[final_columns]
    
    return result_df

def print_preview(df, title="æ•°æ®é¢„è§ˆ", num_rows=10):
    """
    æ‰“å°æ•°æ®é¢„è§ˆ
    """
    print(f"\nğŸ“‹ {title} (å‰{num_rows}è¡Œ):")
    print("=" * 100)
    
    # æ‰“å°åˆ—å
    col_names = list(df.columns)
    col_display = [name[:15].ljust(15) for name in col_names[:8]]  # åªæ˜¾ç¤ºå‰8åˆ—
    print(" | ".join(col_display))
    print("-" * 100)
    
    # æ‰“å°æ•°æ®è¡Œ
    for i in range(min(num_rows, len(df))):
        row = df.iloc[i]
        row_display = []
        for col in col_names[:8]:
            value = str(row[col]) if not pd.isna(row[col]) else ""
            row_display.append(value[:15].ljust(15))
        print(" | ".join(row_display))
    
    print("=" * 100)
    print(f"æ€»è¡Œæ•°: {len(df)} | æ€»åˆ—æ•°: {len(df.columns)}")
    
    # æ˜¾ç¤ºå‰å‡ åˆ—çš„ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        for col in numeric_cols[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ•°å€¼åˆ—
            col_data = df[col].dropna()
            if len(col_data) > 0:
                print(f"  {col:15}: å¹³å‡={col_data.mean():.2f} æœ€å°={col_data.min():.2f} æœ€å¤§={col_data.max():.2f}")

def save_results(df, input_path, output_dir=None):
    """
    ä¿å­˜å¤„ç†ç»“æœ
    """
    if output_dir is None:
        output_dir = os.path.dirname(input_path)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    input_name = os.path.basename(input_path)
    name_without_ext = os.path.splitext(input_name)[0]
    
    csv_output = os.path.join(output_dir, f"{name_without_ext}_processed_{timestamp}.csv")
    json_output = os.path.join(output_dir, f"{name_without_ext}_processed_{timestamp}.json")
    
    # ä¿å­˜CSV
    df.to_csv(csv_output, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ CSVä¿å­˜åˆ°: {csv_output}")
    
    # ä¿å­˜JSONï¼ˆå¯é€‰ï¼‰
    try:
        json_data = df.to_dict(orient='records')
        with open(json_output, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSONä¿å­˜åˆ°: {json_output}")
    except Exception as e:
        print(f"âš   æ— æ³•ä¿å­˜JSON: {e}")
    
    return csv_output, json_output

def process_eod_csv(input_path, output_dir=None, interactive=False):
    """
    ä¸»å¤„ç†å‡½æ•°
    """
    print("=" * 70)
    print("ğŸ¦ EOD CSVä¸“ä¸šå¤„ç†å™¨ - Pythonç‰ˆ")
    print("=" * 70)
    
    # 1. è¯»å–CSVæ–‡ä»¶
    print(f"\nğŸ“ è¯»å–æ–‡ä»¶: {input_path}")
    try:
        # å°è¯•ä¸åŒç¼–ç 
        encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']
        df = None
        
        for encoding in encodings:
            try:
                df = pd.read_csv(input_path, encoding=encoding)
                print(f"âœ… ä½¿ç”¨ç¼–ç : {encoding}")
                break
            except UnicodeDecodeError:
                continue
        
        if df is None:
            raise Exception("æ— æ³•è¯»å–CSVæ–‡ä»¶ï¼Œå°è¯•äº†å¤šç§ç¼–ç ")
        
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None
    
    print(f"ğŸ“Š è¯»å–æˆåŠŸ: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—")
    print("åŸå§‹åˆ—å:", list(df.columns))
    
    # 2. é¢„è§ˆåŸå§‹æ•°æ®
    print_preview(df, "åŸå§‹æ•°æ®")
    
    # 3. è‡ªåŠ¨å¯¹é½åˆ—
    target_order, mapping_info, match_rate = auto_align_columns(df.columns)
    
    print(f"\nğŸ“ˆ åˆ—åŒ¹é…ç‡: {match_rate:.1f}%")
    if match_rate < 70:
        print("âš   è­¦å‘Š: åŒ¹é…ç‡è¾ƒä½ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨è°ƒæ•´")
    
    if interactive:
        print("\nğŸ”§ æ‰‹åŠ¨è°ƒæ•´é€‰é¡¹:")
        print("  1. æ˜¾ç¤ºè¯¦ç»†åŒ¹é…ä¿¡æ¯")
        print("  2. æ‰‹åŠ¨æŒ‡å®šåˆ—æ˜ å°„")
        print("  3. ç»§ç»­å¤„ç†")
        
        choice = input("è¯·é€‰æ‹© (1-3, é»˜è®¤3): ").strip()
        if choice == "1":
            print("\nğŸ“‹ è¯¦ç»†åŒ¹é…ä¿¡æ¯:")
            for info in mapping_info:
                if info['standard'] != "(é¢å¤–)":
                    status_icon = "âœ“" if info['score'] >= 6 else "âš " if info['score'] >= 4 else "âœ—"
                    print(f"  {status_icon} {info['standard']:15} -> {info['actual'] or '[æœªåŒ¹é…]':20} ({info['status']})")
    
    # 4. åº”ç”¨è¡Œä¸šæ˜ å°„
    df = apply_sector_mapping(df)
    
    # 5. é‡æ–°æ’åˆ—åˆ—
    result_df = reorder_dataframe(df, target_order)
    
    # 6. é¢„è§ˆå¤„ç†åçš„æ•°æ®
    print_preview(result_df, "å¤„ç†åçš„æ•°æ®")
    
    # 7. ä¿å­˜ç»“æœ
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    csv_output, json_output = save_results(result_df, input_path, output_dir)
    
    # 8. å®Œæˆ
    print("\n" + "=" * 70)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼")
    print("=" * 70)
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"ğŸ“Š å¤„ç†è¡Œæ•°: {len(df)}")
    print(f"ğŸ“ˆ åˆ—åŒ¹é…ç‡: {match_rate:.1f}%")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {csv_output}")
    if os.path.exists(json_output):
        print(f"ğŸ“„ JSONæ–‡ä»¶: {json_output}")
    print("=" * 70)
    
    return result_df

# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='EOD CSVä¸“ä¸šå¤„ç†å™¨ - åˆ—é‡æ–°æ’åº + è¡Œä¸šä»£ç è½¬æ¢')
    parser.add_argument('input', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ºè¾“å…¥æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼‰')
    parser.add_argument('-i', '--interactive', action='store_true', help='äº¤äº’æ¨¡å¼')
    parser.add_argument('-b', '--batch', help='æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ‰¹é‡å¤„ç†æ¨¡å¼
    if args.batch:
        if not os.path.isdir(args.batch):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.batch}")
            return
        
        csv_files = [f for f in os.listdir(args.batch) if f.lower().endswith('.csv')]
        
        if not csv_files:
            print(f"âŒ åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶: {args.batch}")
            return
        
        print(f"ğŸ” æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        for i, csv_file in enumerate(csv_files, 1):
            input_path = os.path.join(args.batch, csv_file)
            print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ {i}/{len(csv_files)}: {csv_file}")
            
            try:
                process_eod_csv(input_path, args.output, args.interactive)
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        
        print("\nâœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")
    
    # å•æ–‡ä»¶å¤„ç†æ¨¡å¼
    else:
        if not os.path.exists(args.input):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            return
        
        process_eod_csv(args.input, args.output, args.interactive)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâŒ ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
