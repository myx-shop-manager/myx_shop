#!/usr/bin/env python3
"""
======================================================================
ğŸš€ Bursa Malaysia AIé¸è‚¡ç¥å™¨ - å®Œæ•´ç”Ÿç”¢ç‰ˆ (å®‰å…¨JSONç‰ˆæœ¬)
æ•´åˆäº†CSVè¦ç¯„åŒ–åŠŸèƒ½
======================================================================
ğŸ¯ åŠŸèƒ½:
  1. âœ… CSVæ•¸æ“šè¦ç¯„åŒ–ï¼ˆå…¼å®¹å¤šç¨®æ ¼å¼ï¼‰
  2. âœ… ç”Ÿæˆ latest_price.json (webç›®éŒ„) - å®‰å…¨JSONæ ¼å¼
  3. âœ… ç”Ÿæˆ picks_latest.json (webç›®éŒ„) - å®‰å…¨JSONæ ¼å¼
  4. âœ… ç”Ÿæˆ picks_YYYYMMDD.json (web/historyç›®éŒ„)
  5. âœ… è‡ªå‹•å‚™ä»½åˆ°scriptsç›®éŒ„
  6. âœ… è‡ªå‹•æ¸…ç†30å¤©å‰èˆŠæ–‡ä»¶
  7. âœ… å®Œå…¨è™•ç†NaNå€¼ï¼Œç¢ºä¿JSONæœ‰æ•ˆæ€§
======================================================================
"""

import sys
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
import shutil
import csv
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# é…ç½®æ–‡ä»¶è·¯å¾„
# ============================================================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.dirname(SCRIPT_DIR)

# è¾“å‡ºç›®å½•é…ç½®
WEB_DIR = os.path.join(BASE_DIR, "web")
HISTORY_DIR = os.path.join(WEB_DIR, "history")
DATA_DIR = os.path.join(SCRIPT_DIR, "data", "bursa", "picks")
BACKUP_DIR = os.path.join(SCRIPT_DIR, "backups")

# é…ç½®ç›®å½•
CONFIG_DIR = os.path.join(SCRIPT_DIR, "config")
EOD_CONFIG_PATH = os.path.join(CONFIG_DIR, "eod_config.json")

# ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
for directory in [WEB_DIR, HISTORY_DIR, DATA_DIR, BACKUP_DIR, CONFIG_DIR]:
    os.makedirs(directory, exist_ok=True)

# ============================================================================
# CSVè§„èŒƒåŒ–åŠŸèƒ½ï¼ˆæ•´åˆè‡ª normalize_eod.pyï¼‰
# ============================================================================

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("âœ… åˆ›å»ºé»˜è®¤é…ç½®...")
        default_config = {
            "schema": ["code", "name", "last_price", "change", "change_percent", 
                      "volume", "sector", "open", "high", "low", "last_updated"],
            "map": {
                "è‚¡ç¥¨ä»£ç ": "code",
                "è‚¡ç¥¨åç§°": "name",
                "æœ€æ–°ä»·": "last_price",
                "æ¶¨è·Œ": "change",
                "æ¶¨è·Œå¹…": "change_percent",
                "æˆäº¤é‡": "volume",
                "è¡Œä¸š": "sector",
                "å¼€ç›˜": "open",
                "æœ€é«˜": "high",
                "æœ€ä½": "low",
                "æ›´æ–°æ—¶é—´": "last_updated"
            },
            "fill": {
                "sector": "Unknown",
                "last_updated": "15:30:22"
            },
            "sector_lookup": {
                "1": "å·¥ä¸šä¸æ¶ˆè´¹äº§å“",
                "2": "é‡‘è",
                "3": "æˆ¿åœ°äº§",
                "4": "ç§‘æŠ€",
                "5": "èƒ½æº",
                "6": "åŒ»ç–—",
                "7": "è¿è¾“ä¸ç‰©æµ",
                "8": "å»ºç­‘",
                "9": "ç§æ¤",
                "10": "æœåŠ¡"
            }
        }
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(default_config, f, indent=2, ensure_ascii=False)
        return default_config
    
    with open(config_path, "r", encoding="utf-8") as f:
        return json.load(f)

def detect_delimiter(path):
    """æ£€æµ‹CSVæ–‡ä»¶çš„åˆ†éš”ç¬¦"""
    with open(path, "r", newline="", encoding="utf-8") as f:
        sample = f.read(4096)
    
    # ç»Ÿè®¡åˆ¶è¡¨ç¬¦å’Œé€—å·çš„å‡ºç°æ¬¡æ•°
    tab_count = sample.count("\t")
    comma_count = sample.count(",")
    
    # å¦‚æœåˆ¶è¡¨ç¬¦å¤šäºé€—å·ï¼Œåˆ™ä½¿ç”¨åˆ¶è¡¨ç¬¦
    if tab_count > comma_count and tab_count >= 5:  # è‡³å°‘5ä¸ªåˆ¶è¡¨ç¬¦
        return "\t"
    return ","

def normalize_header(header, aliases):
    """è§„èŒƒåŒ–åˆ—å"""
    normalized = []
    for col in header:
        c = col.strip()
        # å°è¯•å¤šç§å¯èƒ½çš„åˆ—åæ ¼å¼
        if c in aliases:
            normalized.append(aliases[c])
        else:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®å­—æ®µ
            if any(keyword in c.lower() for keyword in ["code", "è‚¡ç¥¨ä»£ç ", "stock"]):
                normalized.append("code")
            elif any(keyword in c.lower() for keyword in ["name", "è‚¡ç¥¨åç§°", "stock name"]):
                normalized.append("name")
            elif any(keyword in c.lower() for keyword in ["price", "æœ€æ–°ä»·", "last"]):
                normalized.append("last_price")
            elif any(keyword in c.lower() for keyword in ["change", "æ¶¨è·Œ", "æ¶¨è·Œå¹…"]):
                normalized.append("change_percent")
            elif any(keyword in c.lower() for keyword in ["volume", "æˆäº¤é‡"]):
                normalized.append("volume")
            elif any(keyword in c.lower() for keyword in ["sector", "è¡Œä¸š"]):
                normalized.append("sector")
            else:
                normalized.append(c)
    return normalized

def normalize_csv_file(input_path, output_path=None, config_path=None):
    """
    è§„èŒƒåŒ–CSVæ–‡ä»¶
    è¿”å›è§„èŒƒåŒ–åçš„DataFrameå’Œè¾“å‡ºè·¯å¾„
    """
    if config_path is None:
        config_path = EOD_CONFIG_PATH
    
    config = load_config(config_path)
    schema = config["schema"]
    aliases = config.get("map", {})
    defaults = config.get("fill", {})
    sector_lookup = config.get("sector_lookup", {})
    
    print(f"  ğŸ“ è¾“å…¥æ–‡ä»¶: {input_path}")
    
    try:
        # ç›´æ¥è¯»å–CSV
        df = pd.read_csv(input_path)
    except Exception as e:
        print(f"  âŒ æ— æ³•è¯»å–CSVæ–‡ä»¶: {e}")
        return None, None
    
    print(f"  ğŸ“ˆ è¯»å– {len(df)} è¡Œæ•°æ®")
    print(f"  ğŸ“‹ åŸå§‹åˆ—å: {list(df.columns)}")
    
    # æ¸…ç†åˆ—åï¼šå»é™¤ç©ºæ ¼
    df.columns = [col.strip() for col in df.columns]
    
    # ========== ä¿®å¤åˆ—åæ˜ å°„é—®é¢˜ ==========
    # ç›´æ¥çš„æ‰‹åŠ¨æ˜ å°„ï¼Œé¿å…è‡ªåŠ¨æ˜ å°„å‡ºé”™
    column_mapping = {
        'Code': 'code',
        'Stock': 'name',           # ä¿®å¤ï¼šStockåº”è¯¥æ˜¯nameï¼Œä¸æ˜¯code
        'Sector': 'sector',        # åŸå§‹è¡Œä¸šåˆ—
        'Sector_Code': 'sector_code',  # ä¿®å¤ï¼šé¿å…é‡å¤çš„code
        'Sector_Name': 'sector_name',  # ä¿®å¤ï¼šé¿å…é‡å¤çš„name
        'Last': 'last_price',
        'Open': 'open',
        'High': 'high',
        'Low': 'low',
        'Prv Close': 'prev_close',
        'Chg%': 'change_percent',
        'Vol': 'volume'
    }
    
    # åˆ›å»ºæ–°çš„DataFrame
    normalized_data = {}
    
    for orig_col, norm_col in column_mapping.items():
        if orig_col in df.columns:
            normalized_data[norm_col] = df[orig_col]
            print(f"  âœ… æ˜ å°„ {orig_col} â†’ {norm_col}")
        else:
            print(f"  âš ï¸  åˆ—ä¸å­˜åœ¨: {orig_col}")
    
    # åˆ›å»ºæ–°çš„DataFrame
    normalized_df = pd.DataFrame(normalized_data)
    
    # å¦‚æœsector_nameä¸å­˜åœ¨ï¼Œä½†sectorå­˜åœ¨ï¼Œä½¿ç”¨sector
    if 'sector_name' not in normalized_df.columns and 'sector' in normalized_df.columns:
        normalized_df['sector_name'] = normalized_df['sector']
    
    # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
    required_columns = ['code', 'name', 'last_price', 'change_percent', 
                       'volume', 'sector_name', 'open', 'high', 'low']
    
    for col in required_columns:
        if col not in normalized_df.columns:
            if col == 'change_percent':
                # å°è¯•ä»Chg%è®¡ç®—
                if 'change_percent' not in normalized_df.columns and 'last_price' in normalized_df.columns and 'prev_close' in normalized_df.columns:
                    try:
                        normalized_df['change_percent'] = ((normalized_df['last_price'] - normalized_df['prev_close']) / 
                                                          normalized_df['prev_close'] * 100)
                    except:
                        normalized_df['change_percent'] = 0.0
                else:
                    normalized_df['change_percent'] = 0.0
            elif col in defaults:
                normalized_df[col] = defaults[col]
            else:
                normalized_df[col] = None
    
    # æ·»åŠ æœ€åæ›´æ–°æ—¶é—´
    normalized_df['last_updated'] = datetime.now().strftime('%H:%M:%S')
    
    # è½¬æ¢æ•°å€¼åˆ—
    numeric_columns = ["last_price", "open", "high", "low", "volume", "change_percent"]
    for col in numeric_columns:
        if col in normalized_df.columns:
            try:
                normalized_df[col] = pd.to_numeric(normalized_df[col], errors="coerce")
            except:
                normalized_df[col] = None
    
    # ç”Ÿæˆè¾“å‡ºè·¯å¾„
    if output_path is None:
        input_name = os.path.basename(input_path)
        name_without_ext = os.path.splitext(input_name)[0]
        output_name = f"{name_without_ext}_normalized.csv"
        output_path = os.path.join(os.path.dirname(input_path), output_name)
    
    # ä¿å­˜è§„èŒƒåŒ–åçš„CSV
    normalized_df.to_csv(output_path, index=False, encoding="utf-8")
    print(f"  ğŸ’¾ ä¿å­˜è§„èŒƒåŒ–æ–‡ä»¶: {output_path}")
    print(f"  ğŸ“‹ æœ€ç»ˆåˆ—: {list(normalized_df.columns)}")
    print(f"  ğŸ“Š æ•°æ®è¡Œæ•°: {len(normalized_df)}")
    
    return normalized_df, output_path

# ============================================================================
# AIé¸è‚¡æ ¸å¿ƒåŠŸèƒ½
# ============================================================================

def standardize_columns(df):
    """æ¨™æº–åŒ–åˆ—å"""
    column_mapping = {
        'è‚¡ç¥¨ä»£ç¢¼': 'code',
        'è‚¡ç¥¨åç¨±': 'name',
        'æ”¶ç›¤åƒ¹': 'last_price',
        'é–‹ç›¤åƒ¹': 'open',
        'æœ€é«˜åƒ¹': 'high',
        'æœ€ä½åƒ¹': 'low',
        'æˆäº¤é‡': 'volume',
        'æ¼²è·Œ': 'change',
        'æ¼²è·Œå¹…': 'change_percent',
        'æ¿å¡Š': 'sector',
        'æ›´æ–°æ™‚é–“': 'last_updated'
    }
    
    df = df.rename(columns=column_mapping)
    
    # ç¢ºä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
    required_columns = ['code', 'name', 'last_price', 'open', 'high', 'low', 
                       'volume', 'change', 'change_percent', 'sector']
    
    for col in required_columns:
        if col not in df.columns:
            df[col] = None
    
    return df

def normalize_dataframe(df):
    """æ¨™æº–åŒ–æ•¸æ“šæ¡†ï¼šé‡å‘½ååˆ—ä¸¦è½‰æ›æ•¸æ“šé¡å‹"""
    print("  ğŸ”§ æ¨™æº–åŒ–æ•¸æ“š...")
    
    # å‰µå»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š
    df_norm = df.copy()
    
    # æ¨™æº–åŒ–åˆ—å
    df_norm = standardize_columns(df_norm)
    
    # å˜—è©¦è½‰æ›æ•¸å€¼åˆ—
    numeric_columns = ['last_price', 'open', 'high', 'low', 'volume', 'change', 'change_percent']
    
    for col in numeric_columns:
        if col in df_norm.columns:
            try:
                # é¦–å…ˆç¢ºä¿æ˜¯ Series å°è±¡
                if isinstance(df_norm[col], pd.Series):
                    df_norm[col] = pd.to_numeric(df_norm[col], errors='coerce')
                else:
                    # å¦‚æœä¸æ˜¯ Seriesï¼Œè½‰æ›ç‚º Series
                    df_norm[col] = pd.to_numeric(pd.Series(df_norm[col]), errors='coerce')
                    print(f"    âš ï¸  åˆ— '{col}' å·²å¼·åˆ¶è½‰æ›ç‚º Series")
            except Exception as e:
                print(f"    âŒ ç„¡æ³•è½‰æ›åˆ— '{col}': {e}")
                # å¦‚æœè½‰æ›å¤±æ•—ï¼Œä¿æŒåŸæ¨£
                continue
    
    print("  âœ… æ¨™æº–åŒ–å®Œæˆ")
    return df_norm

def calculate_technical_indicators(df):
    """è¨ˆç®—æŠ€è¡“æŒ‡æ¨™"""
    print("  ğŸ“Š è¨ˆç®—æŠ€è¡“æŒ‡æ¨™...")
    
    df_tech = df.copy()
    
    # ç¢ºä¿æœ‰å¿…è¦çš„åˆ—
    if 'last_price' not in df_tech.columns:
        print("    âš ï¸  ç¼ºå°‘ 'last_price' åˆ—ï¼Œè·³éæŠ€è¡“æŒ‡æ¨™è¨ˆç®—")
        return df_tech
    
    # è¨ˆç®—ç°¡å–®æŠ€è¡“æŒ‡æ¨™
    try:
        # RSI (ç›¸å°å¼·å¼±æŒ‡æ•¸) - ç°¡åŒ–ç‰ˆæœ¬
        price_series = df_tech['last_price'].astype(float)
        
        # è¨ˆç®—åƒ¹æ ¼è®ŠåŒ–
        delta = price_series.diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        
        # è¨ˆç®—å¹³å‡å¢ç›Šå’Œå¹³å‡æå¤±
        avg_gain = gain.rolling(window=14, min_periods=1).mean()
        avg_loss = loss.rolling(window=14, min_periods=1).mean()
        
        # è¨ˆç®—RS
        rs = avg_gain / avg_loss.replace(0, np.nan)
        df_tech['rsi'] = 100 - (100 / (1 + rs))
        
        # ç°¡å–®ç§»å‹•å¹³å‡ç·š
        df_tech['sma_5'] = price_series.rolling(window=5, min_periods=1).mean()
        df_tech['sma_10'] = price_series.rolling(window=10, min_periods=1).mean()
        
        # åƒ¹æ ¼å‹•é‡
        df_tech['momentum'] = price_series.pct_change(periods=5) * 100
        
        print("  âœ… æŠ€è¡“æŒ‡æ¨™è¨ˆç®—å®Œæˆ")
        
    except Exception as e:
        print(f"    âš ï¸  æŠ€è¡“æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {e}")
    
    return df_tech

def ai_scoring(df):
    """AIè©•åˆ†ç³»çµ±"""
    print("  ğŸ§  AIè©•åˆ†ç³»çµ±å•Ÿå‹•...")
    
    scores = []
    
    for idx, row in df.iterrows():
        score = 50  # åŸºç¤åˆ†
        
        try:
            # åƒ¹æ ¼ç›¸é—œè©•åˆ†
            if 'change_percent' in row and pd.notna(row['change_percent']):
                change = float(row['change_percent'])
                if change > 5:
                    score += 15
                elif change > 2:
                    score += 10
                elif change > 0:
                    score += 5
                elif change < -5:
                    score -= 10
                elif change < 0:
                    score -= 5
            
            # æˆäº¤é‡è©•åˆ†
            if 'volume' in row and pd.notna(row['volume']):
                volume = float(row['volume'])
                # ç°¡å–®çš„æˆäº¤é‡è©•åˆ†
                if volume > 1000000:
                    score += 10
                elif volume > 100000:
                    score += 5
                elif volume < 10000:
                    score -= 5
            
            # RSIè©•åˆ†
            if 'rsi' in row and pd.notna(row['rsi']):
                rsi = float(row['rsi'])
                if 30 < rsi < 70:
                    score += 5
                elif rsi < 30:
                    score += 10  # è¶…è³£ï¼Œå¯èƒ½åå½ˆ
                elif rsi > 70:
                    score -= 5   # è¶…è²·
            
            # ç§»å‹•å¹³å‡ç·šè©•åˆ†
            if 'last_price' in row and 'sma_5' in row and pd.notna(row['last_price']) and pd.notna(row['sma_5']):
                price = float(row['last_price'])
                sma5 = float(row['sma_5'])
                if price > sma5:
                    score += 5
            
            # ç¢ºä¿åˆ†æ•¸åœ¨åˆç†ç¯„åœ
            score = max(0, min(100, score))
            
        except Exception as e:
            print(f"    âš ï¸  è‚¡ç¥¨ {row.get('code', 'N/A')} è©•åˆ†éŒ¯èª¤: {e}")
            score = 50
        
        scores.append(score)
    
    df = df.copy()
    df['score'] = scores
    df['score'] = df['score'].round(1)
    
    print("  âœ… AIè©•åˆ†å®Œæˆ")
    return df

def generate_recommendation(row):
    """ç”ŸæˆæŠ•è³‡å»ºè­°"""
    score = row.get('score', 50)
    change = row.get('change_percent', 0)
    rsi = row.get('rsi', 50)
    
    if score >= 80:
        return "ğŸ‘å¼·åŠ›è²·å…¥", "é«˜æ½›åŠ›", "ä½"
    elif score >= 70:
        return "ğŸ‘è²·å…¥", "ä¸­é«˜æ½›åŠ›", "ä¸­ä½"
    elif score >= 60:
        return "ğŸ¤”è€ƒæ…®è²·å…¥", "ä¸­ç­‰æ½›åŠ›", "ä¸­"
    elif score >= 50:
        return "âš–ï¸ä¸­æ€§", "è§€æœ›", "ä¸­é«˜"
    elif score >= 40:
        return "âš ï¸è€ƒæ…®è³£å‡º", "é¢¨éšªåé«˜", "é«˜"
    else:
        return "ğŸš«è³£å‡º", "é«˜é¢¨éšª", "å¾ˆé«˜"

def calculate_potential_score(row):
    """è¨ˆç®—æ½›åŠ›åˆ†æ•¸"""
    score = row.get('score', 50)
    change = row.get('change_percent', 0)
    
    # åŸºç¤æ½›åŠ›åˆ†æ•¸
    potential = score
    
    # æ ¹æ“šæ¼²è·Œå¹…èª¿æ•´
    if change > 5:
        potential += 10
    elif change > 2:
        potential += 5
    elif change < -5:
        potential -= 5
    
    # æ ¹æ“šæˆäº¤é‡èª¿æ•´
    volume = row.get('volume', 0)
    if volume > 500000:
        potential += 5
    
    # é™åˆ¶ç¯„åœ
    potential = max(0, min(100, potential))
    
    return round(potential)

def generate_potential_reasons(row):
    """ç”Ÿæˆæ½›åŠ›åŸå› """
    reasons = []
    
    if row.get('change_percent', 0) > 2:
        reasons.append("åƒ¹æ ¼è¶¨å‹¢å‘ä¸Š")
    
    if row.get('volume', 0) > 100000:
        reasons.append("æˆäº¤é‡æ´»èº")
    
    if row.get('score', 50) > 70:
        reasons.append("AIè©•åˆ†é«˜")
    
    if 'rsi' in row and row['rsi'] < 40:
        reasons.append("RSIé¡¯ç¤ºå¯èƒ½è¶…è³£åå½ˆ")
    elif 'rsi' in row and row['rsi'] > 60:
        reasons.append("RSIé¡¯ç¤ºå¼·å‹¢")
    
    if len(reasons) == 0:
        reasons.append("ç¶œåˆè©•ä¼°ä¸­æ€§")
    
    return "ï¼Œ".join(reasons[:3])

def generate_stock_picks(df, max_picks=20):
    """ç”ŸæˆAIé¸è‚¡æ¸…å–®"""
    print("  ğŸ¯ ç”ŸæˆAIé¸è‚¡æ¸…å–®...")
    
    # è¤‡è£½æ•¸æ“š
    df_picks = df.copy()
    
    # è¨ˆç®—é¡å¤–æŒ‡æ¨™
    df_picks['potential_score'] = df_picks.apply(calculate_potential_score, axis=1)
    
    # ç”Ÿæˆå»ºè­°
    recommendations = df_picks.apply(generate_recommendation, axis=1)
    df_picks['recommendation'] = recommendations.apply(lambda x: x[0])
    df_picks['risk_level'] = recommendations.apply(lambda x: x[2])
    df_picks['status'] = recommendations.apply(lambda x: x[1])
    
    # ç”Ÿæˆæ½›åŠ›åŸå› 
    df_picks['potential_reasons'] = df_picks.apply(generate_potential_reasons, axis=1)
    
    # æ·»åŠ æ¨‚å™¨é¡å‹æª¢æ¸¬ï¼ˆç°¡å–®ç‰ˆæœ¬ï¼‰
    def detect_instrument_type(code):
        if isinstance(code, str):
            if '-' in code or code.endswith(('WA', 'WB', 'WC', 'WR')):
                return "Warrant"
            elif len(code) >= 5 and code[-1].isalpha():
                return "Preference"
        return "Stock"
    
    df_picks['instrument_type'] = df_picks['code'].apply(detect_instrument_type)
    
    # æŒ‰æ½›åŠ›åˆ†æ•¸æ’åº
    df_picks = df_picks.sort_values('potential_score', ascending=False)
    
    # é™åˆ¶æ•¸é‡
    df_picks = df_picks.head(max_picks)
    
    # æ·»åŠ æ’å
    df_picks['rank'] = range(1, len(df_picks) + 1)
    
    # é‡æ–°æ’åˆ—åˆ—
    pick_columns = ['rank', 'code', 'name', 'instrument_type', 'sector',
                   'current_price', 'daily_change', 'score', 'potential_score',
                   'potential_reasons', 'recommendation', 'risk_level',
                   'rsi', 'volume', 'status']
    
    # ç¢ºä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
    for col in pick_columns:
        if col not in df_picks.columns:
            if col == 'current_price':
                df_picks[col] = df_picks.get('last_price', 0)
            elif col == 'daily_change':
                df_picks[col] = df_picks.get('change_percent', 0)
            else:
                df_picks[col] = None
    
    df_picks = df_picks[pick_columns]
    
    print(f"  âœ… ç”Ÿæˆ {len(df_picks)} å€‹é¸è‚¡")
    return df_picks

def save_safe_json(data, filepath, indent=2):
    """å®‰å…¨ä¿å­˜JSONæ–‡ä»¶ï¼Œè™•ç†NaNå€¼"""
    def safe_serializer(obj):
        if isinstance(obj, (np.float32, np.float64)):
            if np.isnan(obj):
                return None
            return float(obj)
        if isinstance(obj, (np.int32, np.int64, np.int8)):
            return int(obj)
        if isinstance(obj, (pd.Timestamp, datetime)):
            return obj.isoformat()
        if pd.isna(obj):
            return None
        raise TypeError(f"ç„¡æ³•åºåˆ—åŒ–é¡å‹: {type(obj)}")
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=indent, default=safe_serializer, ensure_ascii=False)
        print(f"  ğŸ’¾ ä¿å­˜JSONæ–‡ä»¶: {filepath}")
        return True
    except Exception as e:
        print(f"  âŒ ä¿å­˜JSONå¤±æ•— {filepath}: {e}")
        return False

def create_latest_price_json(df, output_dir):
    """å‰µå»ºlatest_price.json"""
    print("  ğŸ“„ å‰µå»º latest_price.json...")
    
    # æº–å‚™æ•¸æ“š
    stocks_list = []
    
    for idx, row in df.iterrows():
        stock_data = {
            'code': str(row.get('code', '')),
            'name': str(row.get('name', '')),
            'last_price': float(row.get('last_price', 0)) if pd.notna(row.get('last_price')) else 0,
            'change': float(row.get('change', 0)) if pd.notna(row.get('change')) else 0,
            'change_percent': float(row.get('change_percent', 0)) if pd.notna(row.get('change_percent')) else 0,
            'volume': int(row.get('volume', 0)) if pd.notna(row.get('volume')) else 0,
            'sector': str(row.get('sector', 'Unknown')),
            'open': float(row.get('open', 0)) if pd.notna(row.get('open')) else 0,
            'high': float(row.get('high', 0)) if pd.notna(row.get('high')) else 0,
            'low': float(row.get('low', 0)) if pd.notna(row.get('low')) else 0,
            'last_updated': str(row.get('last_updated', '15:30:22'))
        }
        stocks_list.append(stock_data)
    
    data = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_date': datetime.now().strftime('%Y-%m-%d'),
        'total_stocks': len(stocks_list),
        'market': 'Bursa Malaysia',
        'stocks': stocks_list
    }
    
    filepath = os.path.join(output_dir, 'latest_price.json')
    if save_safe_json(data, filepath):
        return filepath
    return None

def create_picks_json(df_picks, output_dir, date_str=None):
    """å‰µå»ºé¸è‚¡JSONæ–‡ä»¶"""
    if date_str is None:
        date_str = datetime.now().strftime('%Y%m%d')
    
    print(f"  ğŸ“„ å‰µå»º picks_{date_str}.json...")
    
    # æº–å‚™æ•¸æ“š
    picks_list = []
    
    for idx, row in df_picks.iterrows():
        pick_data = {
            'rank': int(row.get('rank', 0)),
            'code': str(row.get('code', '')),
            'name': str(row.get('name', '')),
            'instrument_type': str(row.get('instrument_type', 'Stock')),
            'sector': str(row.get('sector', '')),
            'current_price': float(row.get('current_price', 0)) if pd.notna(row.get('current_price')) else 0,
            'daily_change': float(row.get('daily_change', 0)) if pd.notna(row.get('daily_change')) else 0,
            'score': float(row.get('score', 0)) if pd.notna(row.get('score')) else 0,
            'potential_score': int(row.get('potential_score', 0)) if pd.notna(row.get('potential_score')) else 0,
            'potential_reasons': str(row.get('potential_reasons', '')),
            'recommendation': str(row.get('recommendation', '')),
            'risk_level': str(row.get('risk_level', '')),
            'rsi': float(row.get('rsi', 0)) if pd.notna(row.get('rsi')) else 0,
            'volume': int(row.get('volume', 0)) if pd.notna(row.get('volume')) else 0,
            'status': str(row.get('status', ''))
        }
        picks_list.append(pick_data)
    
    data = {
        'date': datetime.now().strftime('%Y-%m-%d'),
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'picks': picks_list
    }
    
    filepath = os.path.join(output_dir, f'picks_{date_str}.json')
    if save_safe_json(data, filepath):
        return filepath
    return None

def backup_files(source_dir, backup_dir, prefix="backup_"):
    """å‚™ä»½æ–‡ä»¶"""
    print("  ğŸ’¾ å‰µå»ºå‚™ä»½...")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_path = os.path.join(backup_dir, f"{prefix}{timestamp}")
    
    os.makedirs(backup_path, exist_ok=True)
    
    # å‚™ä»½æ–‡ä»¶
    files_to_backup = ['latest_price.json', 'picks_latest.json']
    
    for filename in files_to_backup:
        source_file = os.path.join(source_dir, filename)
        if os.path.exists(source_file):
            shutil.copy2(source_file, os.path.join(backup_path, filename))
            print(f"    âœ… å‚™ä»½ {filename}")
    
    return backup_path

def cleanup_old_files(directory, days=30):
    """æ¸…ç†èˆŠæ–‡ä»¶"""
    print(f"  ğŸ—‘ï¸  æ¸…ç†{days}å¤©å‰èˆŠæ–‡ä»¶...")
    
    cutoff_date = datetime.now() - timedelta(days=days)
    deleted_count = 0
    
    for filename in os.listdir(directory):
        if filename.startswith('picks_') and filename.endswith('.json'):
            filepath = os.path.join(directory, filename)
            
            # æª¢æŸ¥æ–‡ä»¶ä¿®æ”¹æ™‚é–“
            mod_time = datetime.fromtimestamp(os.path.getmtime(filepath))
            
            if mod_time < cutoff_date:
                os.remove(filepath)
                deleted_count += 1
                print(f"    ğŸ—‘ï¸  åˆªé™¤èˆŠæ–‡ä»¶: {filename}")
    
    if deleted_count > 0:
        print(f"  âœ… åˆªé™¤ {deleted_count} å€‹èˆŠæ–‡ä»¶")
    else:
        print("  âœ… ç„¡éœ€æ¸…ç†")

def main():
    """ä¸»å‡½æ•¸"""
    print("="*70)
    print("ğŸš€ Bursa Malaysia AIé¸è‚¡ç¥å™¨ - å®Œæ•´ç”Ÿç”¢ç‰ˆ (æ•´åˆCSVè¦ç¯„åŒ–)")
    print("="*70)
    print("ğŸ¯ åŠŸèƒ½:")
    print("  1. âœ… CSVæ•¸æ“šè¦ç¯„åŒ–ï¼ˆå…¼å®¹å¤šç¨®æ ¼å¼ï¼‰")
    print("  2. âœ… ç”Ÿæˆ latest_price.json (webç›®éŒ„) - å®‰å…¨JSONæ ¼å¼")
    print("  3. âœ… ç”Ÿæˆ picks_latest.json (webç›®éŒ„) - å®‰å…¨JSONæ ¼å¼")
    print("  4. âœ… ç”Ÿæˆ picks_YYYYMMDD.json (web/historyç›®éŒ„)")
    print("  5. âœ… è‡ªå‹•å‚™ä»½åˆ°scriptsç›®éŒ„")
    print("  6. âœ… è‡ªå‹•æ¸…ç†30å¤©å‰èˆŠæ–‡ä»¶")
    print("  7. âœ… å®Œå…¨è™•ç†NaNå€¼ï¼Œç¢ºä¿JSONæœ‰æ•ˆæ€§")
    print("="*70)
    
    # ç²å–CSVæ–‡ä»¶è·¯å¾‘
    if len(sys.argv) > 1:
        csv_path = sys.argv[1]
    else:
        csv_path = input("è«‹è¼¸å…¥CSVæ–‡ä»¶è·¯å¾‘: ").strip()
    
    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        sys.exit(1)
    
    print(f"\nğŸ“ è¼¸å…¥æ–‡ä»¶: {csv_path}")
    print(f"ğŸ“… æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}")
    print("ğŸ”’ å®‰å…¨æ¨¡å¼: å·²å•Ÿç”¨ï¼ˆå®Œå…¨è™•ç†NaNå€¼ï¼‰")
    
    print("\nğŸ“ å‰µå»ºç›®éŒ„çµæ§‹...")
    for directory in [WEB_DIR, HISTORY_DIR, DATA_DIR, BACKUP_DIR]:
        print(f"    âœ… ç¢ºä¿ç›®éŒ„å­˜åœ¨: {directory}")
    
    # æ­¥é©Ÿ1: CSVè¦ç¯„åŒ–
    print("\nğŸ“Š CSVæ•¸æ“šè¦ç¯„åŒ–...")
    normalized_df, normalized_path = normalize_csv_file(csv_path)
    
    if normalized_df is None:
        print("âŒ CSVè¦ç¯„åŒ–å¤±æ•—")
        sys.exit(1)
    
    # æ­¥é©Ÿ2: æ•¸æ“šæ¨™æº–åŒ–
    print("\nğŸ”§ æ•¸æ“šè™•ç†æµç¨‹...")
    df_standardized = normalize_dataframe(normalized_df)
    
    # æ­¥é©Ÿ3: è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
    df_technical = calculate_technical_indicators(df_standardized)
    
    # æ­¥é©Ÿ4: AIè©•åˆ†
    df_scored = ai_scoring(df_technical)
    
    # æ­¥é©Ÿ5: ç”Ÿæˆé¸è‚¡
    df_picks = generate_stock_picks(df_scored, max_picks=20)
    
    # æ­¥é©Ÿ6: ç”ŸæˆJSONæ–‡ä»¶
    print("\nğŸ’¾ ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶...")
    
    # latest_price.json
    latest_price_file = create_latest_price_json(df_standardized, WEB_DIR)
    
    # picks_latest.json (åœ¨webç›®éŒ„)
    picks_latest_file = create_picks_json(df_picks, WEB_DIR, "latest")
    
    # picks_YYYYMMDD.json (åœ¨historyç›®éŒ„)
    date_str = datetime.now().strftime('%Y%m%d')
    picks_history_file = create_picks_json(df_picks, HISTORY_DIR, date_str)
    
    # å‚™ä»½
    backup_path = backup_files(WEB_DIR, BACKUP_DIR)
    
    # æ¸…ç†èˆŠæ–‡ä»¶
    cleanup_old_files(HISTORY_DIR, days=30)
    
    # ç¸½çµ
    print("\n" + "="*70)
    print("ğŸ‰ AIé¸è‚¡å®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“Š è¼¸å…¥æ•¸æ“š: {len(df_standardized)} æ”¯è‚¡ç¥¨")
    print(f"ğŸ¯ AIé¸è‚¡: {len(df_picks)} å€‹æ¨è–¦")
    print(f"ğŸ’¾ ç”Ÿæˆæ–‡ä»¶:")
    print(f"   1. {latest_price_file if latest_price_file else 'latest_price.json (å¤±æ•—)'}")
    print(f"   2. {picks_latest_file if picks_latest_file else 'picks_latest.json (å¤±æ•—)'}")
    print(f"   3. {picks_history_file if picks_history_file else f'picks_{date_str}.json (å¤±æ•—)'}")
    print(f"   4. å‚™ä»½: {backup_path}")
    print(f"\nâ° ä¸‹æ¬¡é‹è¡Œ: python ai_stock_picker_full.py [CSVæ–‡ä»¶è·¯å¾‘]")
    print("="*70)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâŒ ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)