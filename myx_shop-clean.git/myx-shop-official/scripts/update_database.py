#!/usr/bin/env python3
"""
ä¸»æ•¸æ“šæ›´æ–°è…³æœ¬ - æ•´åˆæ‰€æœ‰æ­¥é©Ÿ
"""
import subprocess
import os
import json
import schedule
import time
from datetime import date, timedelta
import sys

class DataPipeline:
    def __init__(self):
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.config_dir = os.path.join(self.base_dir, 'config')
        self.data_dir = os.path.join(self.base_dir, 'data')
        
        # ç›®éŒ„è·¯å¾‘
        self.dirs = {
            'raw': os.path.join(self.data_dir, 'raw'),
            'normalized': os.path.join(self.data_dir, 'normalized'),
            'picks': os.path.join(self.data_dir, 'picks'),
            'audit': os.path.join(self.data_dir, 'audit'),
            'reports': os.path.join(self.data_dir, 'reports'),
            'logs': os.path.join(self.base_dir, 'logs')
        }
        
        # å‰µå»ºæ‰€æœ‰ç›®éŒ„
        for dir_path in self.dirs.values():
            os.makedirs(dir_path, exist_ok=True)
        
        # é…ç½®æ–‡ä»¶è·¯å¾‘
        self.config_file = os.path.join(self.config_dir, 'eod_config.json')
        self.sector_lookup_file = os.path.join(self.config_dir, 'sector_lookup.json')
        
        # æª¢æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.config_file):
            self.create_default_config()
    
    def create_default_config(self):
        """å‰µå»ºé»˜èªé…ç½®"""
        default_config = {
            "schema": [
                "Date",
                "Code",
                "Name", 
                "Open",
                "High",
                "Low",
                "Close",
                "Volume",
                "Value",
                "Change",
                "Change%",
                "Sector Code",
                "Sector"
            ],
            "map": {
                "è‚¡ç¥¨ä»£ç¢¼": "Code",
                "åç¨±": "Name",
                "é–‹ç›¤": "Open",
                "æœ€é«˜": "High",
                "æœ€ä½": "Low",
                "æ”¶ç›¤": "Close",
                "æˆäº¤é‡": "Volume",
                "æˆäº¤å€¼": "Value",
                "æ¼²è·Œ": "Change",
                "æ¼²è·Œå¹…": "Change%",
                "è¡Œæ¥­ä»£ç¢¼": "Sector Code"
            },
            "fill": {
                "Date": "-",
                "Sector": "Unknown"
            }
        }
        
        os.makedirs(self.config_dir, exist_ok=True)
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å·²å‰µå»ºé»˜èªé…ç½®: {self.config_file}")
    
    def run_pipeline(self, target_date=None):
        """é‹è¡Œå®Œæ•´æ•¸æ“šæµæ°´ç·š"""
        if target_date is None:
            target_date = date.today()
        
        date_str = target_date.strftime("%Y-%m-%d")
        print(f"\n{'='*60}")
        print(f"ğŸš€ é‹è¡Œæ•¸æ“šæµæ°´ç·š - {date_str}")
        print(f"{'='*60}")
        
        # æ­¥é©Ÿ 1: ä¸‹è¼‰åŸå§‹æ•¸æ“š
        print("\nğŸ“¥ æ­¥é©Ÿ 1: ä¸‹è¼‰åŸå§‹ EOD æ•¸æ“š")
        raw_file = self.download_raw_data(target_date)
        if not raw_file:
            print("âŒ ä¸‹è¼‰å¤±æ•—ï¼Œè·³éå¾ŒçºŒæ­¥é©Ÿ")
            return False
        
        # æ­¥é©Ÿ 2: æ¨™æº–åŒ–è™•ç†
        print("\nğŸ”§ æ­¥é©Ÿ 2: æ¨™æº–åŒ– EOD æ•¸æ“š")
        normalized_file = self.normalize_data(raw_file, target_date)
        if not normalized_file:
            print("âŒ æ¨™æº–åŒ–å¤±æ•—ï¼Œè·³éå¾ŒçºŒæ­¥é©Ÿ")
            return False
        
        # æ­¥é©Ÿ 3: ç”Ÿæˆ AI åˆ†æ
        print("\nğŸ¤– æ­¥é©Ÿ 3: ç”Ÿæˆ AI æ¨è–¦")
        picks_file = self.generate_picks(normalized_file, target_date)
        
        # æ­¥é©Ÿ 4: æ›´æ–°æœ€æ–°æ¨è–¦
        print("\nğŸ”— æ­¥é©Ÿ 4: æ›´æ–°æœ€æ–°æ¨è–¦")
        if picks_file:
            self.update_latest_picks(picks_file)
        
        # æ­¥é©Ÿ 5: æ›´æ–°æ—¥æœŸç´¢å¼•
        print("\nğŸ“Š æ­¥é©Ÿ 5: æ›´æ–°æ—¥æœŸç´¢å¼•")
        self.update_dates_index()
        
        print(f"\nâœ… æµæ°´ç·šå®Œæˆ: {date_str}")
        return True
    
    def download_raw_data(self, target_date):
        """ä¸‹è¼‰åŸå§‹æ•¸æ“š"""
        try:
            # èª¿ç”¨ download_eod.py
            download_script = os.path.join(self.base_dir, 'scripts', 'download_eod.py')
            
            if not os.path.exists(download_script):
                print("âŒ download_eod.py ä¸å­˜åœ¨")
                return None
            
            # ä½¿ç”¨å­é€²ç¨‹é‹è¡Œ
            result = subprocess.run(
                [sys.executable, download_script],
                capture_output=True,
                text=True,
                encoding='utf-8'
            )
            
            if result.returncode == 0:
                date_str = target_date.strftime("%Y-%m-%d")
                raw_file = os.path.join(self.dirs['raw'], f"{date_str}_raw.csv")
                
                if os.path.exists(raw_file):
                    print(f"âœ… åŸå§‹æ•¸æ“š: {raw_file}")
                    return raw_file
                else:
                    print("âŒ åŸå§‹æ–‡ä»¶æœªæ‰¾åˆ°")
                    return None
            else:
                print(f"âŒ ä¸‹è¼‰å¤±æ•—: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"âŒ ä¸‹è¼‰éŒ¯èª¤: {e}")
            return None
    
    def normalize_data(self, raw_file, target_date):
        """æ¨™æº–åŒ–æ•¸æ“š"""
        try:
            date_str = target_date.strftime("%Y-%m-%d")
            normalized_file = os.path.join(self.dirs['normalized'], f"{date_str}.csv")
            audit_file = os.path.join(self.dirs['audit'], f"{date_str}_audit.json")
            
            # æ§‹å»º normalize_eod.py å‘½ä»¤
            normalize_script = os.path.join(self.base_dir, 'scripts', 'normalize_eod.py')
            
            cmd = [
                sys.executable,
                normalize_script,
                raw_file,
                normalized_file,
                self.config_file,
                audit_file
            ]
            
            # å¦‚æœæœ‰è¡Œæ¥­æ˜ å°„æ–‡ä»¶ï¼Œæ·»åŠ åˆ°é…ç½®
            if os.path.exists(self.sector_lookup_file):
                # è®€å–è¡Œæ¥­æ˜ å°„ä¸¦åˆä½µåˆ°é…ç½®
                with open(self.sector_lookup_file, 'r', encoding='utf-8') as f:
                    sector_lookup = json.load(f)
                
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                config['sector_lookup'] = sector_lookup
                
                # å‰µå»ºè‡¨æ™‚é…ç½®
                temp_config = os.path.join(self.dirs['logs'], 'temp_config.json')
                with open(temp_config, 'w', encoding='utf-8') as f:
                    json.dump(config, f, ensure_ascii=False, indent=2)
                
                cmd[3] = temp_config  # æ›¿æ›é…ç½®è·¯å¾‘
            
            # é‹è¡Œæ¨™æº–åŒ–è…³æœ¬
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding='utf-8'
            )
            
            if result.returncode == 0:
                print(f"âœ… æ¨™æº–åŒ–å®Œæˆ: {normalized_file}")
                
                # æª¢æŸ¥å¯©è¨ˆæ—¥èªŒ
                if os.path.exists(audit_file):
                    with open(audit_file, 'r', encoding='utf-8') as f:
                        audit = json.load(f)
                    print(f"  è™•ç†è¨˜éŒ„: {audit['rows_in']} -> {audit['rows_out']} è¡Œ")
                
                return normalized_file
            else:
                print(f"âŒ æ¨™æº–åŒ–å¤±æ•—: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"âŒ æ¨™æº–åŒ–éŒ¯èª¤: {e}")
            return None
    
    def generate_picks(self, normalized_file, target_date):
        """ç”Ÿæˆ AI æ¨è–¦"""
        try:
            date_str = target_date.strftime("%Y-%m-%d")
            
            # é‹è¡Œ generate_picks.py
            picks_script = os.path.join(self.base_dir, 'scripts', 'generate_picks.py')
            
            cmd = [
                sys.executable,
                picks_script,
                '--date', date_str,
                '--input', normalized_file,
                '--output', self.dirs['picks']
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding='utf-8'
            )
            
            if result.returncode == 0:
                picks_file = os.path.join(self.dirs['picks'], f"{date_str}.json")
                print(f"âœ… AI æ¨è–¦ç”Ÿæˆ: {picks_file}")
                return picks_file
            else:
                print(f"âŒ AI æ¨è–¦ç”Ÿæˆå¤±æ•—: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"âŒ AI æ¨è–¦éŒ¯èª¤: {e}")
            return None
    
    def update_latest_picks(self, picks_file):
        """æ›´æ–°æœ€æ–°æ¨è–¦æ–‡ä»¶"""
        try:
            # è®€å–ç”Ÿæˆçš„ JSON
            with open(picks_file, 'r', encoding='utf-8') as f:
                picks_data = json.load(f)
            
            # æ·»åŠ é¡å¤–ä¿¡æ¯
            picks_data['data_status'] = 'LIVE'
            picks_data['update_timestamp'] = time.time()
            picks_data['pipeline_version'] = '2.0'
            
            # ä¿å­˜ç‚º picks.jsonï¼ˆå‰ç«¯ä½¿ç”¨ï¼‰
            latest_file = os.path.join(self.base_dir, 'picks.json')
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(picks_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ”— æ›´æ–°æœ€æ–°æ¨è–¦: {latest_file}")
            
        except Exception as e:
            print(f"âŒ æ›´æ–°æœ€æ–°æ¨è–¦å¤±æ•—: {e}")
    
    def update_dates_index(self):
        """æ›´æ–°æ—¥æœŸç´¢å¼•"""
        try:
            dates_script = os.path.join(self.base_dir, 'scripts', 'generate_dates_index.py')
            
            result = subprocess.run(
                [sys.executable, dates_script],
                capture_output=True,
                text=True,
                encoding='utf-8'
            )
            
            if result.returncode == 0:
                print("âœ… æ—¥æœŸç´¢å¼•æ›´æ–°å®Œæˆ")
            else:
                print(f"âŒ æ—¥æœŸç´¢å¼•æ›´æ–°å¤±æ•—: {result.stderr}")
                
        except Exception as e:
            print(f"âŒ æ—¥æœŸç´¢å¼•éŒ¯èª¤: {e}")
    
    def run_historical_pipeline(self, start_date, end_date):
        """é‹è¡Œæ­·å²æ•¸æ“šæµæ°´ç·š"""
        current_date = start_date
        success_count = 0
        
        print(f"\nğŸ“… è™•ç†æ­·å²æ•¸æ“š: {start_date} è‡³ {end_date}")
        
        while current_date <= end_date:
            # è·³éå‘¨æœ«
            if current_date.weekday() < 5:
                print(f"\nè™•ç† {current_date.strftime('%Y-%m-%d')}...")
                success = self.run_pipeline(current_date)
                if success:
                    success_count += 1
            
            current_date += timedelta(days=1)
            time.sleep(0.5)  # é¿å…éè¼‰
        
        print(f"\nğŸ‰ æ­·å²æ•¸æ“šè™•ç†å®Œæˆ: {success_count} å¤©æˆåŠŸ")
        return success_count
    
    def run_scheduler(self):
        """é‹è¡Œå®šæ™‚ä»»å‹™"""
        print("â° å•Ÿå‹•æ•¸æ“šæµæ°´ç·šå®šæ™‚ä»»å‹™...")
        print(f"  æ¯æ—¥é‹è¡Œæ™‚é–“: 18:30 (æ”¶ç›¤å¾Œ)")
        print(f"  æ•¸æ“šç›®éŒ„: {self.data_dir}")
        
        # è¨­å®šå®šæ™‚ä»»å‹™
        schedule.every().day.at("18:30").do(self.run_pipeline)
        
        # å‘¨ä¸€é¡å¤–è™•ç†ä¸Šå‘¨äº”æ•¸æ“š
        schedule.every().monday.at("09:00").do(
            lambda: self.run_pipeline(date.today() - timedelta(days=3))
        )
        
        # ç«‹å³åŸ·è¡Œä¸€æ¬¡
        self.run_pipeline()
        
        # ä¿æŒé‹è¡Œ
        while True:
            schedule.run_pending()
            time.sleep(60)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='é‹è¡Œæ•¸æ“šæµæ°´ç·š')
    parser.add_argument('--date', help='è™•ç†æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--historical', action='store_true', help='è™•ç†æ­·å²æ•¸æ“š')
    parser.add_argument('--start', help='æ­·å²æ•¸æ“šé–‹å§‹æ—¥æœŸ')
    parser.add_argument('--end', help='æ­·å²æ•¸æ“šçµæŸæ—¥æœŸ')
    parser.add_argument('--daemon', action='store_true', help='é‹è¡Œå®ˆè­·é€²ç¨‹')
    
    args = parser.parse_args()
    
    pipeline = DataPipeline()
    
    if args.daemon:
        # é‹è¡Œå®ˆè­·é€²ç¨‹æ¨¡å¼
        pipeline.run_scheduler()
    
    elif args.historical:
        # è™•ç†æ­·å²æ•¸æ“š
        start_date = date.fromisoformat(args.start) if args.start else date.today() - timedelta(days=30)
        end_date = date.fromisoformat(args.end) if args.end else date.today()
        
        pipeline.run_historical_pipeline(start_date, end_date)
    
    elif args.date:
        # è™•ç†æŒ‡å®šæ—¥æœŸ
        target_date = date.fromisoformat(args.date)
        pipeline.run_pipeline(target_date)
    
    else:
        # è™•ç†ä»Šå¤©
        pipeline.run_pipeline()
